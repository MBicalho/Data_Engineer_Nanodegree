<h3 align="center">Spark and Data Lakes</h3>
<p align="center">
 Udacity Data Engineer Nanodegree Course Project 4
 <br />
</p>


# About the Project

In this project, you'll apply what you've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, you will need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. You'll deploy this Spark process on a cluster using AWS.

Neste projeto, iremos aplicar o que aprendemos no Spark e nos data lakes para criar um pipeline ETL para um data lake hospedado no S3. Carregaremos os dados do S3, processaremos os dados em tabelas de análise usando o Spark e iremos carregá-los novamente no S3. Será implantado esse processo do Spark em um cluster usando a AWS.

# Project Description

A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

Uma startup de streaming de música, Sparkify, aumentou ainda mais sua base de usuários e banco de dados de músicas e deseja mover seu data warehouse para um data lake. Seus dados residem no S3, em um diretório de logs JSON sobre a atividade do usuário no aplicativo, bem como em um diretório com metadados JSON nas músicas em seu aplicativo.

Como engenheiro de dados, você tem a tarefa de criar um pipeline de ETL que extrai seus dados do S3, os processa usando o Spark e carrega os dados de volta no S3 como um conjunto de tabelas dimensionais. Isso permitirá que a equipe de análise continue encontrando informações sobre quais músicas seus usuários estão ouvindo.

# Tools Used

* Python
* PostgreSQL
* Jupyter notebooks
* Apache Spark
* AWS

# Datasets

Two datasets residing on S3 were used.
* Song data: ```s3://udacity-dend/song_data```
* Song data: ```s3://udacity-dend/log_data```

Log data json path: ```s3://udacity-dend/log_json_path.json```

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are file paths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```


Foi utlizado dois conjuntos de dados que residem no S3.
* Song data: ```s3://udacity-dend/song_data```
* Song data: ```s3://udacity-dend/log_data```

Log data json path: ```s3://udacity-dend/log_json_path.json```

O primeiro conjunto de dados é um subconjunto de dados reais do [Million Song Dataset](http://millionsongdataset.com/). Cada arquivo está no formato JSON e contém metadados sobre uma música e o artista dessa música. Os arquivos são particionados pelas três primeiras letras do ID da faixa de cada música. Por exemplo, aqui estão os caminhos de arquivo para dois arquivos neste conjunto de dados.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
E abaixo está um exemplo da aparência de um único arquivo de música, TRAABJL12903CDCF1A.json.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

O segundo conjunto de dados consiste em arquivos de log no formato JSON gerados por este simulador de eventos com base nas músicas do conjunto de dados acima. Eles simulam os logs de atividade do aplicativo de um aplicativo de streaming de música imaginário com base nas configurações.

Os arquivos de log no conjunto de dados com os quais você trabalhará são particionados por ano e mês. Por exemplo, aqui estão os caminhos de arquivo para dois arquivos neste conjunto de dados.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

# Data Modeling

The data stored in S3 will be extracted and we will populate our tables through an ETL through Spark, after that we will store the tables again in an S3 through parquet files

Os dados armazenados no S3 serão extraídos e popularemos nossas tabelas através de um ETL através do Spark, após isso iremos armazenar novamente as tabelas em um S3 através de arquivos de parquet.



# Project Structure

|File/Folder| File/Folder Description |
| --- | --- |
| data | Folder with the root of the project  |
| etl.py | File Python with the etl from S3 buckets to the tables |
| dl.cfg | File with the configurations of the AWS |
| README.md | Readme |

# How to Execute

Clone the repository on your machine run through the command.
```git clone https://github.com/MBicalho/Data_Engineer_Nanodegree.git```

# Tools to Execute the project

* Python
* AWS Services
* Spark

# Step by Step

* Create the bucket in Amazon

* Configure the file ```dl.cfg``` with the credentials and the url bucket

* Run the file ```etl.py``` for etl process


# Contact
Matheus Bicalho [mbicalho.freitas@gmail.com]
Linkedin: [https://www.linkedin.com/in/matheus-bicalho-0a5835205/]
